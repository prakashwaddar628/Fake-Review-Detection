{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.18.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: C:\\Users\\praka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text, X_test_text, y_train, y_test, vectorizer = pd.read_pickle(\"../datasets/processed_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 89.70%\n",
      "Logistic Regression Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90      4071\n",
      "           1       0.90      0.89      0.90      4016\n",
      "\n",
      "    accuracy                           0.90      8087\n",
      "   macro avg       0.90      0.90      0.90      8087\n",
      "weighted avg       0.90      0.90      0.90      8087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression(max_iter=500)\n",
    "log_model.fit(X_train_text, y_train)\n",
    "log_y_pred = log_model.predict(X_test_text)\n",
    "log_accuracy = accuracy_score(y_test, log_y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {log_accuracy * 100:.2f}%\")\n",
    "print(\"Logistic Regression Report:\\n\", classification_report(y_test, log_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 88.35%\n",
      "Random Forest Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88      4071\n",
      "           1       0.88      0.89      0.88      4016\n",
      "\n",
      "    accuracy                           0.88      8087\n",
      "   macro avg       0.88      0.88      0.88      8087\n",
      "weighted avg       0.88      0.88      0.88      8087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_text, y_train)\n",
    "rf_y_pred = rf_model.predict(X_test_text)\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy * 100:.2f}%\")\n",
    "print(\"Random Forest Report:\\n\", classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes Accuracy: 86.97%\n",
      "Naïve Bayes Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.84      0.87      4071\n",
      "           1       0.85      0.90      0.87      4016\n",
      "\n",
      "    accuracy                           0.87      8087\n",
      "   macro avg       0.87      0.87      0.87      8087\n",
      "weighted avg       0.87      0.87      0.87      8087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_text, y_train)\n",
    "nb_y_pred = nb_model.predict(X_test_text)\n",
    "nb_accuracy = accuracy_score(y_test, nb_y_pred)\n",
    "print(f\"Naïve Bayes Accuracy: {nb_accuracy * 100:.2f}%\")\n",
    "print(\"Naïve Bayes Report:\\n\", classification_report(y_test, nb_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 5000\n",
    "MAX_SEQUENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse\n",
    "\n",
    "if issparse(X_train_text):\n",
    "    X_train_text = X_train_text.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix to text\n",
    "X_train_text = vectorizer.inverse_transform(X_train_text)\n",
    "X_train_text = [\" \".join(words) for words in X_train_text]  # Convert list of word arrays into sentences\n",
    "\n",
    "X_test_text = vectorizer.inverse_transform(X_test_text)\n",
    "X_test_text = [\" \".join(words) for words in X_test_text]  # Same for test data\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X_train_text)  \n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)  \n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text)  \n",
    "\n",
    "# Pad sequences\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential([\n",
    "    Embedding(MAX_NUM_WORDS, 128, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 122ms/step - accuracy: 0.8078 - loss: 0.3881 - val_accuracy: 0.8885 - val_loss: 0.2639\n",
      "Epoch 2/5\n",
      "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 140ms/step - accuracy: 0.9225 - loss: 0.1909 - val_accuracy: 0.8924 - val_loss: 0.2567\n",
      "Epoch 3/5\n",
      "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 190ms/step - accuracy: 0.9376 - loss: 0.1525 - val_accuracy: 0.8841 - val_loss: 0.2768\n",
      "Epoch 4/5\n",
      "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 185ms/step - accuracy: 0.9506 - loss: 0.1254 - val_accuracy: 0.8784 - val_loss: 0.3112\n",
      "Epoch 5/5\n",
      "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 174ms/step - accuracy: 0.9588 - loss: 0.1052 - val_accuracy: 0.8692 - val_loss: 0.3525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22f0b820620>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(X_train_pad, np.array(y_train), epochs=5, batch_size=32, validation_data=(X_test_pad, np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.8733 - loss: 0.3486\n",
      "LSTM Model Accuracy: 86.92%\n"
     ]
    }
   ],
   "source": [
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test_pad, np.array(y_test))\n",
    "print(f\"LSTM Model Accuracy: {lstm_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 5005\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary Size: {len(tokenizer.word_index)}\")\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=X_train_pad.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pad = vectorizer.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.50%\n"
     ]
    }
   ],
   "source": [
    "accuracy = best_model.score(X_test_pad, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.88      4071\n",
      "           1       0.93      0.79      0.85      4016\n",
      "\n",
      "    accuracy                           0.86      8087\n",
      "   macro avg       0.87      0.86      0.86      8087\n",
      "weighted avg       0.87      0.86      0.86      8087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = best_model.predict(X_test_pad)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_test_text_reset = pd.Series(X_test_text).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 1, Predicted: 0\n",
      "Review: would with will when way was wants wait very two to this the strong story so she series see romance review relationship recommend received real reading read plot pace out on of next my mother more meets man main loved love looking look know it is in how honest him her happy happily handsome great good get fun from free forward for flowed first find ever escape doesn do development definitely copy comes characters character can by but book between be author at anything anyone and an after\n",
      "--------------------------------------------------------------------------------\n",
      "Actual: 1, Predicted: 0\n",
      "Review: your yet year with will white use trailer to this then the so see provide problem over or option on off of not no meant long light last it issues is in have had goes for feet enough does dimmable color changing change camping bulb bright blue any and again\n",
      "--------------------------------------------------------------------------------\n",
      "Actual: 1, Predicted: 0\n",
      "Review: with well was ve turn told to this the story still since series robert researched recommend really reader read others one of not mother little life lee lacking kid it is into in hits highly have has grandson from forces follows father fan eye events dramatic disappointed different detailed detail child but bull books book best been and an always account\n",
      "--------------------------------------------------------------------------------\n",
      "Actual: 1, Predicted: 0\n",
      "Review: you year would with which water was very using use too to this they these them the that tea sturdy stay so slide seal rubber rolls really push purchase pour pleased pitcher out our other or old off of nuts now not no need month matter make love like kinds keeps just it is into if hot homemade holding have happy handle great getting from for fact expecting eggs easy ease dry don doesn color coffee clean chicken challenge buy but bright bowls bowl bottom biscuit been be baking as are and am always allows again\n",
      "--------------------------------------------------------------------------------\n",
      "Actual: 1, Predicted: 0\n",
      "Review: with waist the short shirts quality part little is hit great good front fit but boys big and\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Find misclassified indices\n",
    "misclassified_idx = np.where(y_test != y_pred)[0]\n",
    "\n",
    "# Reset index of y_test\n",
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "\n",
    "# Print misclassified examples\n",
    "for idx in misclassified_idx[:5]:  \n",
    "    print(f\"Actual: {y_test_reset[idx]}, Predicted: {y_pred[idx]}\")  \n",
    "    print(f\"Review: {X_test_text[idx]}\")  # No need for .reset_index()\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = max(\n",
    "    [(log_model, log_accuracy), (rf_model, rf_accuracy), (nb_model, nb_accuracy)], key=lambda x: x[1]\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lstm_accuracy > max(log_accuracy, rf_accuracy, nb_accuracy):\n",
    "    lstm_model.save(\"../models/lstm_model.h5\")\n",
    "    best_model = \"LSTM Model\"\n",
    "else:\n",
    "    with open(\"../models/model.pkl\", \"wb\") as model_file:\n",
    "        pickle.dump(best_model, model_file)\n",
    "    with open(\"../models/vectorizer.pkl\", \"wb\") as vectorizer_file:\n",
    "        pickle.dump(vectorizer, vectorizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model selected: LogisticRegression(max_iter=500)\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model selected: {best_model}\")\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
